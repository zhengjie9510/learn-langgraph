{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "假设你有一组文档（如PDF、Notion页面、客户问题等），并希望对其内容进行总结。\n",
    "\n",
    "大型语言模型（LLMs）在理解和综合文本方面表现出色，因此非常适合用来进行总结。\n",
    "\n",
    "在检索增强生成（retrieval-augmented generation）的应用场景中，文本总结有助于提炼从大量检索到的文档中获取的信息，为LLM提供背景上下文。\n",
    "\n",
    "在本文中，我们将介绍如何使用LLMs对多个文档的内容进行总结。\n",
    "\n",
    "![](https://python.langchain.com/assets/images/summarization_use_case_1-874f7b2c94f64216f1f967fb5aca7bc1.png)\n",
    "\n",
    "# 概述\n",
    "在构建总结器时，一个核心问题是如何将文档内容传入LLM的上下文窗口。有两种常见的方法：\n",
    "\n",
    "Stuff（填充法）：将所有文档直接“填充”到一个单一的提示中。这是最简单的方法（更多关于此方法的信息可以参考create_stuff_documents_chain构造器的说明）。\n",
    "\n",
    "Map-reduce（映射-归约法）：在“映射”步骤中先分别对每个文档进行总结，然后将这些总结“归约”为一个最终的总结（更多关于此方法的信息可以参考MapReduceDocumentsChain的说明）。\n",
    "\n",
    "需要注意的是，map-reduce方法尤其在理解子文档内容不依赖于前后文时非常有效。例如，在总结一组较短的文档时。如果是像总结小说或具有内在序列关系的文本，[逐步精炼](https://python.langchain.com/docs/how_to/summarize_refine/)的方法可能会更加有效。\n",
    "\n",
    "![](https://python.langchain.com/assets/images/summarization_use_case_2-f2a4d5d60980a79140085fb7f8043217.png)"
   ],
   "id": "80672697ad780ec1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from config import *\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='qwen-max')"
   ],
   "id": "3283e77bfcf4d9f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()"
   ],
   "id": "b9cc898750ad1bf9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 填充法：在一次LLM调用中总结\n",
    "我们可以使用 create_stuff_documents_chain 方法，特别是当使用具有更大上下文窗口的模型时，例如：\n",
    "\n",
    "128k token 的 OpenAI GPT-4o\n",
    "200k token 的 Anthropic Claude-3-5-sonnet-20240620\n",
    "该方法会将文档列表传入，将所有文档插入到一个提示中，然后将这个提示传递给LLM进行处理。"
   ],
   "id": "21b681a57575548a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"Write a concise summary of the following:\\\\n\\\\n{context}\")]\n",
    ")\n",
    "\n",
    "# Instantiate chain\n",
    "chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# Invoke chain\n",
    "result = chain.invoke({\"context\": docs})\n",
    "print(result)"
   ],
   "id": "43e9728f2c891774",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 流式处理\n",
    "需要注意的是，我们还可以将结果按令牌逐个流式传输："
   ],
   "id": "3bdf29102d42c84a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for token in chain.stream({\"context\": docs}):\n",
    "    print(token, end=\"|\")"
   ],
   "id": "f17dfde497644a95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 映射-归约法：通过并行化总结长文本\n",
    "我们来详细讲解一下映射-归约法。首先，我们会使用LLM将每个文档映射为一个独立的总结。然后，我们将这些总结进行归约或整合，最终得到一个全局总结。\n",
    "\n",
    "需要注意的是，映射步骤通常会并行化处理输入的文档。\n",
    "\n",
    "LangGraph，基于 langchain-core 构建，支持映射-归约工作流，非常适合解决这个问题\n",
    "\n",
    "## 映射步骤\n",
    "首先，我们需要定义与映射步骤相关的提示。我们可以使用与填充法（Stuff 方法）中相同的总结提示，如下所示："
   ],
   "id": "cbb608aafcafadae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "map_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"Write a concise summary of the following:\\\\n\\\\n{context}\")]\n",
    ")"
   ],
   "id": "da174459f8c76225",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "我们还可以使用 Prompt Hub 来存储和获取提示。",
   "id": "b3d56f3422c477a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain import hub\n",
    "\n",
    "map_prompt = hub.pull(\"rlm/map-prompt\")\n",
    "map_prompt"
   ],
   "id": "9f6c5e3d0e2b44c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 归约步骤\n",
    "我们还需要定义一个提示，接收文档映射的结果，并将它们归约为一个单一的输出。"
   ],
   "id": "17f8606e3767d241"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "reduce_template = \"\"\"\n",
    "The following is a set of summaries:\n",
    "{docs}\n",
    "Take these and distill it into a final, consolidated summary\n",
    "of the main themes.\n",
    "\"\"\"\n",
    "\n",
    "reduce_prompt = ChatPromptTemplate([(\"human\", reduce_template)])"
   ],
   "id": "da659c651ace12cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 使用 LangGraph 进行 orchestration（协调）\n",
    "下面我们实现一个简单的应用程序，首先在文档列表上进行总结映射步骤，然后使用上述提示将它们归约成一个最终输出。\n",
    "\n",
    "映射-归约流程在文本较长、而LLM的上下文窗口较小时尤其有用。对于较长的文本，我们需要一个机制来确保在归约步骤中，待总结的上下文不会超过模型的上下文窗口大小。\n",
    "\n",
    "首先，我们将博客文章划分成更小的“子文档”以供映射处理："
   ],
   "id": "d460a1375548e6cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=0\n",
    ")\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "print(f\"Generated {len(split_docs)} documents.\")"
   ],
   "id": "69f073046493859a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "接下来，定义图（Graph）。",
   "id": "ae9ffb9a8998ae9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import operator\n",
    "from typing import Annotated, List, Literal, TypedDict\n",
    "\n",
    "from langchain.chains.combine_documents.reduce import (\n",
    "    acollapse_docs,\n",
    "    split_list_of_docs,\n",
    ")\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.constants import Send\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "\n",
    "token_max = 1000\n",
    "\n",
    "\n",
    "def length_function(documents: List[Document]) -> int:\n",
    "    \"\"\"Get number of tokens for input contents.\"\"\"\n",
    "    return sum(llm.get_num_tokens(doc.page_content) for doc in documents)\n",
    "\n",
    "\n",
    "# This will be the overall state of the main graph.\n",
    "# It will contain the input document contents, corresponding\n",
    "# summaries, and a final summary.\n",
    "class OverallState(TypedDict):\n",
    "    # Notice here we use the operator.add\n",
    "    # This is because we want combine all the summaries we generate\n",
    "    # from individual nodes back into one list - this is essentially\n",
    "    # the \"reduce\" part\n",
    "    contents: List[str]\n",
    "    summaries: Annotated[list, operator.add]\n",
    "    collapsed_summaries: List[Document]\n",
    "    final_summary: str\n",
    "\n",
    "\n",
    "# This will be the state of the node that we will \"map\" all\n",
    "# documents to in order to generate summaries\n",
    "class SummaryState(TypedDict):\n",
    "    content: str\n",
    "\n",
    "\n",
    "# Here we generate a summary, given a document\n",
    "async def generate_summary(state: SummaryState):\n",
    "    prompt = map_prompt.invoke(state[\"content\"])\n",
    "    response = await llm.ainvoke(prompt)\n",
    "    return {\"summaries\": [response.content]}\n",
    "\n",
    "\n",
    "# Here we define the logic to map out over the documents\n",
    "# We will use this an edge in the graph\n",
    "def map_summaries(state: OverallState):\n",
    "    # We will return a list of `Send` objects\n",
    "    # Each `Send` object consists of the name of a node in the graph\n",
    "    # as well as the state to send to that node\n",
    "    return [\n",
    "        Send(\"generate_summary\", {\"content\": content}) for content in state[\"contents\"]\n",
    "    ]\n",
    "\n",
    "\n",
    "def collect_summaries(state: OverallState):\n",
    "    return {\n",
    "        \"collapsed_summaries\": [Document(summary) for summary in state[\"summaries\"]]\n",
    "    }\n",
    "\n",
    "\n",
    "async def _reduce(input: dict) -> str:\n",
    "    prompt = reduce_prompt.invoke(input)\n",
    "    response = await llm.ainvoke(prompt)\n",
    "    return response.content\n",
    "\n",
    "\n",
    "# Add node to collapse summaries\n",
    "async def collapse_summaries(state: OverallState):\n",
    "    doc_lists = split_list_of_docs(\n",
    "        state[\"collapsed_summaries\"], length_function, token_max\n",
    "    )\n",
    "    results = []\n",
    "    for doc_list in doc_lists:\n",
    "        results.append(await acollapse_docs(doc_list, _reduce))\n",
    "\n",
    "    return {\"collapsed_summaries\": results}\n",
    "\n",
    "\n",
    "# This represents a conditional edge in the graph that determines\n",
    "# if we should collapse the summaries or not\n",
    "def should_collapse(\n",
    "        state: OverallState,\n",
    ") -> Literal[\"collapse_summaries\", \"generate_final_summary\"]:\n",
    "    num_tokens = length_function(state[\"collapsed_summaries\"])\n",
    "    if num_tokens > token_max:\n",
    "        return \"collapse_summaries\"\n",
    "    else:\n",
    "        return \"generate_final_summary\"\n",
    "\n",
    "\n",
    "# Here we will generate the final summary\n",
    "async def generate_final_summary(state: OverallState):\n",
    "    response = await _reduce(state[\"collapsed_summaries\"])\n",
    "    return {\"final_summary\": response}\n",
    "\n",
    "\n",
    "# Construct the graph\n",
    "# Nodes:\n",
    "graph = StateGraph(OverallState)\n",
    "graph.add_node(\"generate_summary\", generate_summary)  # same as before\n",
    "graph.add_node(\"collect_summaries\", collect_summaries)\n",
    "graph.add_node(\"collapse_summaries\", collapse_summaries)\n",
    "graph.add_node(\"generate_final_summary\", generate_final_summary)\n",
    "\n",
    "# Edges:\n",
    "graph.add_conditional_edges(START, map_summaries, [\"generate_summary\"])\n",
    "graph.add_edge(\"generate_summary\", \"collect_summaries\")\n",
    "graph.add_conditional_edges(\"collect_summaries\", should_collapse)\n",
    "graph.add_conditional_edges(\"collapse_summaries\", should_collapse)\n",
    "graph.add_edge(\"generate_final_summary\", END)\n",
    "\n",
    "app = graph.compile()"
   ],
   "id": "92c194800ed5ceeb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ],
   "id": "89b3b48eacfd7ac4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "在运行应用程序时，我们可以流式输出图的执行过程，以观察它的步骤顺序。下面，我们将简单地打印出每个步骤的名称。\n",
    "\n",
    "需要注意的是，由于图中有循环，因此在执行时指定 recursion_limit 参数会很有帮助。当超过指定的递归限制时，它会抛出一个特定的错误。"
   ],
   "id": "53c0c355fdc25af7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "async for step in app.astream(\n",
    "        {\"contents\": [doc.page_content for doc in split_docs]},\n",
    "        {\"recursion_limit\": 2},\n",
    "):\n",
    "    print(list(step.keys()))"
   ],
   "id": "8ec1addd67957ce3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
