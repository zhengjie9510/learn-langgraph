{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-25T08:50:19.773672Z",
     "start_time": "2025-02-25T08:50:18.943147Z"
    }
   },
   "source": [
    "from config import *\n",
    "\n",
    "import bs4\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing_extensions import List, TypedDict, Annotated\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AnyMessage, trim_messages"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhengjie/Documents/learn-langchain/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 获取网页内容\n",
    "\n",
    "这一部分我们将从网页中提取文本，然后将其分成小块，以便逐步翻译。"
   ],
   "id": "84166d95996f8d12"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T08:50:20.493931Z",
     "start_time": "2025-02-25T08:50:19.786370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bs4_strainer = bs4.SoupStrainer(class_=(\"md-content__inner md-typeset\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://langchain-ai.github.io/langgraph/tutorials/chatbots/information-gather-prompting/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,  # chunk size (characters)\n",
    "    chunk_overlap=0,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    "\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)"
   ],
   "id": "c963bf2fd9191aad",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 建立翻译机制\n",
    "\n",
    "我们将使用一个简单的状态机来处理翻译任务。我们将从一个系统消息开始，制定翻译规则，然后逐步翻译文档的每个部分。"
   ],
   "id": "d6de36fbbb6ab1bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T08:50:20.602984Z",
     "start_time": "2025-02-25T08:50:20.600069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = SystemMessage(\"您是一名精通多领域的技术文档翻译专家，需要将英文技术文档翻译成中文。\\n\"\n",
    "                       \"你需要遵循以下规则:\\n\"\n",
    "                       \"1. 使用Markdown格式对文档进行排版，各级标题需要使用相应的格式（如“#”、“##”），这一点很重要。\\n\"\n",
    "                       \"2. 有关代码的部分不需要进行翻译，包括代码的注释。\\n\"\n",
    "                       \"3. 只需要翻译我给你的内容，不要去创作、不要闲聊。\\n\"\n",
    "                       \"4. 请尽量保持翻译的准确性和流畅性。\\n\"\n",
    "                       \"最后，再次提醒，翻译的结果要用Markdown格式。\\n\")"
   ],
   "id": "cb5b243ff852a714",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T08:50:21.343522Z",
     "start_time": "2025-02-25T08:50:21.340413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class State(TypedDict):\n",
    "    num: int\n",
    "    translated_splits: Annotated[List[AnyMessage], add_messages]\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "    translated: str"
   ],
   "id": "d9f784cbac95a19c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T08:50:22.311538Z",
     "start_time": "2025-02-25T08:50:22.305650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ask_for_translation(state: State):\n",
    "    return {\n",
    "        \"messages\": [HumanMessage(all_splits[state['num']].page_content)]\n",
    "    }\n",
    "\n",
    "\n",
    "def translate(state: State):\n",
    "    messages = trim_messages(state['messages'],\n",
    "                             max_tokens=3,\n",
    "                             token_counter=len,\n",
    "                             start_on=\"human\",\n",
    "                             end_on=\"human\")\n",
    "\n",
    "    # 每次传递的消息都是从当前状态开始（包含了 prompt 和之前的翻译）\n",
    "    messages = [prompt] + messages\n",
    "\n",
    "    # 获取模型的响应\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    # 修复：确保我们只将当前翻译的结果返回，而不重复之前的翻译\n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "        \"translated_splits\": [response],  # 只返回当前分块的翻译\n",
    "        \"num\": state['num'] + 1  # 更新索引，确保不会重复处理已翻译的分块\n",
    "    }\n",
    "\n",
    "\n",
    "def collect(state: State):\n",
    "    return {\n",
    "        \"translated\": \"\\n\".join([m.content for m in state['translated_splits']])\n",
    "    }\n",
    "\n",
    "\n",
    "def route(state: State):\n",
    "    # 当所有分块都已翻译时，结束处理\n",
    "    if state['num'] < len(all_splits):\n",
    "        return \"ask_for_translation\"\n",
    "    else:\n",
    "        return \"collect\""
   ],
   "id": "2d06e4d52183ab32",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "构建状态图，添加节点和边",
   "id": "e43182d37abc1509"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T08:50:30.340249Z",
     "start_time": "2025-02-25T08:50:29.797073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 初始化 LLM 模型\n",
    "llm = ChatOpenAI(model=\"qwen-max\")\n",
    "\n",
    "# 创建状态图\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"ask_for_translation\", ask_for_translation)\n",
    "graph_builder.add_node(\"translate\", translate)\n",
    "graph_builder.add_node(\"collect\", collect)\n",
    "\n",
    "# 定义节点之间的流动关系\n",
    "graph_builder.add_edge(START, \"ask_for_translation\")\n",
    "graph_builder.add_edge(\"ask_for_translation\", \"translate\")\n",
    "graph_builder.add_conditional_edges(\"translate\", route,\n",
    "                                    {\"ask_for_translation\": \"ask_for_translation\",\n",
    "                                     \"collect\": \"collect\"})\n",
    "graph_builder.add_edge(\"collect\", END)\n",
    "\n",
    "# 编译状态图\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))"
   ],
   "id": "151495ca516fcde7",
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 展示翻译结果",
   "id": "b4302f3eef098d91"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T08:52:54.882558Z",
     "start_time": "2025-02-25T08:50:35.471348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 流式执行并打印每个步骤的输出\n",
    "inputs = {\"num\": 0, \"translated_splits\": [], \"messages\": []}\n",
    "result = graph.invoke(inputs)"
   ],
   "id": "c22fb0719005ffc6",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T08:53:13.352845Z",
     "start_time": "2025-02-25T08:53:13.341338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for message in result[\"messages\"]:\n",
    "    message.pretty_print()"
   ],
   "id": "d4ce671ac4b4f9e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001B[1m Human Message \u001B[0m=================================\n",
      "\n",
      "Prompt Generation from User Requirements¶\n",
      "In this example we will create a chat bot that helps a user generate a prompt.\n",
      "It will first collect requirements from the user, and then will generate the prompt (and refine it based on user input).\n",
      "These are split into two separate states, and the LLM decides when to transition between them.\n",
      "A graphical representation of the system can be found below.\n",
      "\n",
      "Setup¶\n",
      "First, let's install our required packages and set our OpenAI API key (the LLM we will use)\n",
      "%%capture --no-stderr\n",
      "% pip install -U langgraph langchain_openai\n",
      "\n",
      "import getpass\n",
      "import os\n",
      "\n",
      "\n",
      "def _set_env(var: str):\n",
      "    if not os.environ.get(var):\n",
      "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
      "\n",
      "\n",
      "_set_env(\"OPENAI_API_KEY\")\n",
      "\n",
      "\n",
      "Set up LangSmith for LangGraph development\n",
      "\n",
      "        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started here. \n",
      "    \n",
      "\n",
      "Gather information¶\n",
      "First, let's define the part of the graph that will gather user requirements. This will be an LLM call with a specific system message. It will have access to a tool that it can call when it is ready to generate the prompt.\n",
      "\n",
      "Using Pydantic with LangChain\n",
      "\n",
      "        This notebook uses Pydantic v2 BaseModel, which requires langchain-core >= 0.3. Using langchain-core < 0.3 will result in errors due to mixing of Pydantic v1 and v2 BaseModels.\n",
      "    \n",
      "\n",
      "from typing import List\n",
      "\n",
      "from langchain_core.messages import SystemMessage\n",
      "from langchain_openai import ChatOpenAI\n",
      "\n",
      "from pydantic import BaseModel\n",
      "\n",
      "API Reference: SystemMessage | ChatOpenAI\n",
      "template = \"\"\"Your job is to get information from a user about what type of prompt template they want to create.\n",
      "\n",
      "You should get the following information from them:\n",
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "# 从用户需求生成提示\n",
      "在这个示例中，我们将创建一个聊天机器人，帮助用户生成一个提示。\n",
      "它首先会从用户那里收集需求，然后生成提示（并根据用户输入进行优化）。\n",
      "这些过程被分为两个独立的状态，而LLM将决定何时在这两者之间切换。\n",
      "下面是一个系统的图形表示。\n",
      "\n",
      "## 设置\n",
      "首先，让我们安装所需的包并设置我们的OpenAI API密钥（我们将使用的LLM）\n",
      "```python\n",
      "%%capture --no-stderr\n",
      "% pip install -U langgraph langchain_openai\n",
      "\n",
      "import getpass\n",
      "import os\n",
      "\n",
      "\n",
      "def _set_env(var: str):\n",
      "    if not os.environ.get(var):\n",
      "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
      "\n",
      "\n",
      "_set_env(\"OPENAI_API_KEY\")\n",
      "```\n",
      "\n",
      "### 设置LangSmith以支持LangGraph开发\n",
      "注册LangSmith可以快速发现问题并改进您的LangGraph项目。LangSmith允许您使用跟踪数据来调试、测试和监控使用LangGraph构建的LLM应用程序 —— [了解更多关于如何开始的信息](这里)。\n",
      "\n",
      "## 收集信息\n",
      "首先，让我们定义图的一部分，该部分将收集用户需求。这将是一个带有特定系统消息的LLM调用。它将能够访问一个工具，在准备好生成提示时可以调用该工具。\n",
      "\n",
      "### 使用Pydantic与LangChain\n",
      "本笔记本使用了Pydantic v2的BaseModel，这需要langchain-core >= 0.3。如果使用langchain-core < 0.3，则会导致Pydantic v1和v2 BaseModel混合导致错误。\n",
      "\n",
      "```python\n",
      "from typing import List\n",
      "\n",
      "from langchain_core.messages import SystemMessage\n",
      "from langchain_openai import ChatOpenAI\n",
      "\n",
      "from pydantic import BaseModel\n",
      "\n",
      "API参考: SystemMessage | ChatOpenAI\n",
      "template = \"\"\"Your job is to get information from a user about what type of prompt template they want to create.\n",
      "\n",
      "You should get the following information from them:\n",
      "\"\"\"\n",
      "```\n",
      "================================\u001B[1m Human Message \u001B[0m=================================\n",
      "\n",
      "- What the objective of the prompt is\n",
      "- What variables will be passed into the prompt template\n",
      "- Any constraints for what the output should NOT do\n",
      "- Any requirements that the output MUST adhere to\n",
      "\n",
      "If you are not able to discern this info, ask them to clarify! Do not attempt to wildly guess.\n",
      "\n",
      "After you are able to discern all the information, call the relevant tool.\"\"\"\n",
      "\n",
      "\n",
      "def get_messages_info(messages):\n",
      "    return [SystemMessage(content=template)] + messages\n",
      "\n",
      "\n",
      "class PromptInstructions(BaseModel):\n",
      "    \"\"\"Instructions on how to prompt the LLM.\"\"\"\n",
      "\n",
      "    objective: str\n",
      "    variables: List[str]\n",
      "    constraints: List[str]\n",
      "    requirements: List[str]\n",
      "\n",
      "\n",
      "llm = ChatOpenAI(temperature=0)\n",
      "llm_with_tool = llm.bind_tools([PromptInstructions])\n",
      "\n",
      "\n",
      "def info_chain(state):\n",
      "    messages = get_messages_info(state[\"messages\"])\n",
      "    response = llm_with_tool.invoke(messages)\n",
      "    return {\"messages\": [response]}\n",
      "\n",
      "Generate Prompt¶\n",
      "We now set up the state that will generate the prompt.\n",
      "This will require a separate system message, as well as a function to filter out all message PRIOR to the tool invocation (as that is when the previous state decided it was time to generate the prompt\n",
      "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage\n",
      "\n",
      "# New system prompt\n",
      "prompt_system = \"\"\"Based on the following requirements, write a good prompt template:\n",
      "\n",
      "{reqs}\"\"\"\n",
      "\n",
      "\n",
      "# Function to get the messages for the prompt\n",
      "# Will only get messages AFTER the tool call\n",
      "def get_prompt_messages(messages: list):\n",
      "    tool_call = None\n",
      "    other_msgs = []\n",
      "    for m in messages:\n",
      "        if isinstance(m, AIMessage) and m.tool_calls:\n",
      "            tool_call = m.tool_calls[0][\"args\"]\n",
      "        elif isinstance(m, ToolMessage):\n",
      "            continue\n",
      "        elif tool_call is not None:\n",
      "            other_msgs.append(m)\n",
      "    return [SystemMessage(content=prompt_system.format(reqs=tool_call))] + other_msgs\n",
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "- 提示的目标是什么\n",
      "- 将传递给提示模板的变量有哪些\n",
      "- 输出不应该做什么的任何限制\n",
      "- 输出必须遵守的任何要求\n",
      "\n",
      "如果您无法明确这些信息，请让他们澄清！不要随意猜测。\n",
      "\n",
      "在您能够明确所有信息后，调用相关的工具。\n",
      "\n",
      "\n",
      "```python\n",
      "def get_messages_info(messages):\n",
      "    return [SystemMessage(content=template)] + messages\n",
      "\n",
      "\n",
      "class PromptInstructions(BaseModel):\n",
      "    \"\"\"关于如何向LLM发出提示的说明。\"\"\"\n",
      "\n",
      "    objective: str\n",
      "    variables: List[str]\n",
      "    constraints: List[str]\n",
      "    requirements: List[str]\n",
      "\n",
      "\n",
      "llm = ChatOpenAI(temperature=0)\n",
      "llm_with_tool = llm.bind_tools([PromptInstructions])\n",
      "\n",
      "\n",
      "def info_chain(state):\n",
      "    messages = get_messages_info(state[\"messages\"])\n",
      "    response = llm_with_tool.invoke(messages)\n",
      "    return {\"messages\": [response]}\n",
      "```\n",
      "\n",
      "## 生成提示\n",
      "我们现在设置将生成提示的状态。\n",
      "这需要一个单独的系统消息，以及一个函数来过滤掉工具调用之前的所有消息（因为这是前一个状态决定生成提示的时候）。\n",
      "\n",
      "```python\n",
      "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage\n",
      "\n",
      "# 新的系统提示\n",
      "prompt_system = \"\"\"基于以下需求，编写一个好的提示模板：\n",
      "\n",
      "{reqs}\"\"\"\n",
      "\n",
      "\n",
      "# 获取提示消息的函数\n",
      "# 只获取工具调用之后的消息\n",
      "def get_prompt_messages(messages: list):\n",
      "    tool_call = None\n",
      "    other_msgs = []\n",
      "    for m in messages:\n",
      "        if isinstance(m, AIMessage) and m.tool_calls:\n",
      "            tool_call = m.tool_calls[0][\"args\"]\n",
      "        elif isinstance(m, ToolMessage):\n",
      "            continue\n",
      "        elif tool_call is not None:\n",
      "            other_msgs.append(m)\n",
      "    return [SystemMessage(content=prompt_system.format(reqs=tool_call))] + other_msgs\n",
      "```\n",
      "================================\u001B[1m Human Message \u001B[0m=================================\n",
      "\n",
      "def prompt_gen_chain(state):\n",
      "    messages = get_prompt_messages(state[\"messages\"])\n",
      "    response = llm.invoke(messages)\n",
      "    return {\"messages\": [response]}\n",
      "\n",
      "API Reference: AIMessage | HumanMessage | ToolMessage\n",
      "Define the state logic¶\n",
      "This is the logic for what state the chatbot is in.\n",
      "If the last message is a tool call, then we are in the state where the \"prompt creator\" (prompt) should respond.\n",
      "Otherwise, if the last message is not a HumanMessage, then we know the human should respond next and so we are in the END state.\n",
      "If the last message is a HumanMessage, then if there was a tool call previously we are in the prompt state.\n",
      "Otherwise, we are in the \"info gathering\" (info) state.\n",
      "from typing import Literal\n",
      "\n",
      "from langgraph.graph import END\n",
      "\n",
      "\n",
      "def get_state(state):\n",
      "    messages = state[\"messages\"]\n",
      "    if isinstance(messages[-1], AIMessage) and messages[-1].tool_calls:\n",
      "        return \"add_tool_message\"\n",
      "    elif not isinstance(messages[-1], HumanMessage):\n",
      "        return END\n",
      "    return \"info\"\n",
      "\n",
      "API Reference: END\n",
      "Create the graph¶\n",
      "We can now the create the graph.\n",
      "We will use a SqliteSaver to persist conversation history.\n",
      "from langgraph.checkpoint.memory import MemorySaver\n",
      "from langgraph.graph import StateGraph, START\n",
      "from langgraph.graph.message import add_messages\n",
      "from typing import Annotated\n",
      "from typing_extensions import TypedDict\n",
      "\n",
      "\n",
      "class State(TypedDict):\n",
      "    messages: Annotated[list, add_messages]\n",
      "\n",
      "\n",
      "memory = MemorySaver()\n",
      "workflow = StateGraph(State)\n",
      "workflow.add_node(\"info\", info_chain)\n",
      "workflow.add_node(\"prompt\", prompt_gen_chain)\n",
      "\n",
      "\n",
      "@workflow.add_node\n",
      "def add_tool_message(state: State):\n",
      "    return {\n",
      "        \"messages\": [\n",
      "            ToolMessage(\n",
      "                content=\"Prompt generated!\",\n",
      "                tool_call_id=state[\"messages\"][-1].tool_calls[0][\"id\"],\n",
      "            )\n",
      "        ]\n",
      "    }\n",
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "```python\n",
      "def prompt_gen_chain(state):\n",
      "    messages = get_prompt_messages(state[\"messages\"])\n",
      "    response = llm.invoke(messages)\n",
      "    return {\"messages\": [response]}\n",
      "\n",
      "# API 参考: AIMessage | HumanMessage | ToolMessage\n",
      "## 定义状态逻辑\n",
      "这是聊天机器人所处状态的逻辑。\n",
      "如果最后一条消息是工具调用，那么我们处于“提示创建者”（prompt）应该响应的状态。\n",
      "否则，如果最后一条消息不是HumanMessage，那么我们知道接下来应该是人类响应，所以我们处于END状态。\n",
      "如果最后一条消息是HumanMessage，那么如果有之前的工具调用，我们就处于提示状态。\n",
      "否则，我们处于“信息收集”（info）状态。\n",
      "\n",
      "from typing import Literal\n",
      "\n",
      "from langgraph.graph import END\n",
      "\n",
      "\n",
      "def get_state(state):\n",
      "    messages = state[\"messages\"]\n",
      "    if isinstance(messages[-1], AIMessage) and messages[-1].tool_calls:\n",
      "        return \"add_tool_message\"\n",
      "    elif not isinstance(messages[-1], HumanMessage):\n",
      "        return END\n",
      "    return \"info\"\n",
      "\n",
      "# API 参考: END\n",
      "## 创建图\n",
      "我们现在可以创建图。\n",
      "我们将使用SqliteSaver来持久化对话历史记录。\n",
      "from langgraph.checkpoint.memory import MemorySaver\n",
      "from langgraph.graph import StateGraph, START\n",
      "from langgraph.graph.message import add_messages\n",
      "from typing import Annotated\n",
      "from typing_extensions import TypedDict\n",
      "\n",
      "\n",
      "class State(TypedDict):\n",
      "    messages: Annotated[list, add_messages]\n",
      "\n",
      "\n",
      "memory = MemorySaver()\n",
      "workflow = StateGraph(State)\n",
      "workflow.add_node(\"info\", info_chain)\n",
      "workflow.add_node(\"prompt\", prompt_gen_chain)\n",
      "\n",
      "\n",
      "@workflow.add_node\n",
      "def add_tool_message(state: State):\n",
      "    return {\n",
      "        \"messages\": [\n",
      "            ToolMessage(\n",
      "                content=\"Prompt generated!\",\n",
      "                tool_call_id=state[\"messages\"][-1].tool_calls[0][\"id\"],\n",
      "            )\n",
      "        ]\n",
      "    }\n",
      "```\n",
      "================================\u001B[1m Human Message \u001B[0m=================================\n",
      "\n",
      "workflow.add_conditional_edges(\"info\", get_state, [\"add_tool_message\", \"info\", END])\n",
      "workflow.add_edge(\"add_tool_message\", \"prompt\")\n",
      "workflow.add_edge(\"prompt\", END)\n",
      "workflow.add_edge(START, \"info\")\n",
      "graph = workflow.compile(checkpointer=memory)\n",
      "\n",
      "API Reference: MemorySaver | StateGraph | START | add_messages\n",
      "from IPython.display import Image, display\n",
      "\n",
      "display(Image(graph.get_graph().draw_mermaid_png()))\n",
      "\n",
      "\n",
      "Use the graph¶\n",
      "We can now use the created chatbot.\n",
      "import uuid\n",
      "\n",
      "cached_human_responses = [\"hi!\", \"rag prompt\", \"1 rag, 2 none, 3 no, 4 no\", \"red\", \"q\"]\n",
      "cached_response_index = 0\n",
      "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
      "while True:\n",
      "    try:\n",
      "        user = input(\"User (q/Q to quit): \")\n",
      "    except:\n",
      "        user = cached_human_responses[cached_response_index]\n",
      "        cached_response_index += 1\n",
      "    print(f\"User (q/Q to quit): {user}\")\n",
      "    if user in {\"q\", \"Q\"}:\n",
      "        print(\"AI: Byebye\")\n",
      "        break\n",
      "    output = None\n",
      "    for output in graph.stream(\n",
      "        {\"messages\": [HumanMessage(content=user)]}, config=config, stream_mode=\"updates\"\n",
      "    ):\n",
      "        last_message = next(iter(output.values()))[\"messages\"][-1]\n",
      "        last_message.pretty_print()\n",
      "\n",
      "    if output and \"prompt\" in output:\n",
      "        print(\"Done!\")\n",
      "\n",
      "User (q/Q to quit): hi!\n",
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "Hello! How can I assist you today?\n",
      "User (q/Q to quit): rag prompt\n",
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "Sure! I can help you create a prompt template. To get started, could you please provide me with the following information:\n",
      "\n",
      "1. What is the objective of the prompt?\n",
      "2. What variables will be passed into the prompt template?\n",
      "3. Any constraints for what the output should NOT do?\n",
      "4. Any requirements that the output MUST adhere to?\n",
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "```markdown\n",
      "workflow.add_conditional_edges(\"info\", get_state, [\"add_tool_message\", \"info\", END])\n",
      "workflow.add_edge(\"add_tool_message\", \"prompt\")\n",
      "workflow.add_edge(\"prompt\", END)\n",
      "workflow.add_edge(START, \"info\")\n",
      "graph = workflow.compile(checkpointer=memory)\n",
      "\n",
      "# API 参考: MemorySaver | StateGraph | START | add_messages\n",
      "from IPython.display import Image, display\n",
      "\n",
      "display(Image(graph.get_graph().draw_mermaid_png()))\n",
      "\n",
      "## 使用图\n",
      "我们现在可以使用创建的聊天机器人。\n",
      "import uuid\n",
      "\n",
      "cached_human_responses = [\"hi!\", \"rag prompt\", \"1 rag, 2 none, 3 no, 4 no\", \"red\", \"q\"]\n",
      "cached_response_index = 0\n",
      "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
      "while True:\n",
      "    try:\n",
      "        user = input(\"用户 (q/Q 退出): \")\n",
      "    except:\n",
      "        user = cached_human_responses[cached_response_index]\n",
      "        cached_response_index += 1\n",
      "    print(f\"用户 (q/Q 退出): {user}\")\n",
      "    if user in {\"q\", \"Q\"}:\n",
      "        print(\"AI: 再见\")\n",
      "        break\n",
      "    output = None\n",
      "    for output in graph.stream(\n",
      "        {\"messages\": [HumanMessage(content=user)]}, config=config, stream_mode=\"updates\"\n",
      "    ):\n",
      "        last_message = next(iter(output.values()))[\"messages\"][-1]\n",
      "        last_message.pretty_print()\n",
      "\n",
      "    if output and \"prompt\" in output:\n",
      "        print(\"完成!\")\n",
      "\n",
      "用户 (q/Q 退出): hi!\n",
      "==================================\u001B[1m Ai 消息 \u001B[0m==================================\n",
      "\n",
      "你好！今天我能帮你什么？\n",
      "\n",
      "用户 (q/Q 退出): rag prompt\n",
      "==================================\u001B[1m Ai 消息 \u001B[0m==================================\n",
      "\n",
      "当然！我可以帮助你创建一个提示模板。首先，请提供以下信息：\n",
      "\n",
      "1. 提示的目标是什么？\n",
      "2. 将传递到提示模板中的变量有哪些？\n",
      "3. 输出不应做什么的任何约束条件？\n",
      "4. 输出必须遵守的任何要求？\n",
      "```\n",
      "================================\u001B[1m Human Message \u001B[0m=================================\n",
      "\n",
      "Once I have this information, I can assist you in creating the prompt template.\n",
      "User (q/Q to quit): 1 rag, 2 none, 3 no, 4 no\n",
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "Tool Calls:\n",
      "  PromptInstructions (call_tcz0foifsaGKPdZmsZxNnepl)\n",
      " Call ID: call_tcz0foifsaGKPdZmsZxNnepl\n",
      "  Args:\n",
      "    objective: rag\n",
      "    variables: ['none']\n",
      "    constraints: ['no']\n",
      "    requirements: ['no']\n",
      "=================================\u001B[1m Tool Message \u001B[0m=================================\n",
      "\n",
      "Prompt generated!\n",
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "Please write a response using the RAG (Red, Amber, Green) rating system.\n",
      "Done!\n",
      "User (q/Q to quit): red\n",
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "Response: The status is RED.\n",
      "User (q/Q to quit): q\n",
      "AI: Byebye\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Was this page helpful?\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "              \n",
      "              \n",
      "                \n",
      "              \n",
      "              Thanks for your feedback!\n",
      "            \n",
      "\n",
      "              \n",
      "              \n",
      "                \n",
      "              \n",
      "              Thanks for your feedback! Please help us improve this page by adding to the discussion below.\n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Comments\n",
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "```markdown\n",
      "一旦我获得了这些信息，我就可以帮助你创建提示模板。\n",
      "用户 (q/Q 退出): 1 rag, 2 none, 3 no, 4 no\n",
      "==================================\u001B[1m Ai 消息 \u001B[0m==================================\n",
      "工具调用:\n",
      "  PromptInstructions (call_tcz0foifsaGKPdZmsZxNnepl)\n",
      " 调用 ID: call_tcz0foifsaGKPdZmsZxNnepl\n",
      "  参数:\n",
      "    目标: rag\n",
      "    变量: ['none']\n",
      "    约束条件: ['no']\n",
      "    要求: ['no']\n",
      "=================================\u001B[1m 工具消息 \u001B[0m=================================\n",
      "\n",
      "提示已生成！\n",
      "==================================\u001B[1m Ai 消息 \u001B[0m==================================\n",
      "\n",
      "请使用 RAG（红色、琥珀色、绿色）评级系统编写回复。\n",
      "完成!\n",
      "用户 (q/Q 退出): red\n",
      "==================================\u001B[1m Ai 消息 \u001B[0m==================================\n",
      "\n",
      "回复: 状态为红色。\n",
      "用户 (q/Q 退出): q\n",
      "AI: 再见\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        这个页面有帮助吗？\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "              \n",
      "              \n",
      "                \n",
      "              \n",
      "              感谢您的反馈！\n",
      "            \n",
      "\n",
      "              \n",
      "              \n",
      "                \n",
      "              \n",
      "              感谢您的反馈！请通过在下面的讨论中添加内容来帮助我们改进此页面。\n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "评论\n",
      "```\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T08:53:23.899994Z",
     "start_time": "2025-02-25T08:53:23.894600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(result[\"translated\"]))"
   ],
   "id": "7e59dad8004d7673",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "# 从用户需求生成提示\n在这个示例中，我们将创建一个聊天机器人，帮助用户生成一个提示。\n它首先会从用户那里收集需求，然后生成提示（并根据用户输入进行优化）。\n这些过程被分为两个独立的状态，而LLM将决定何时在这两者之间切换。\n下面是一个系统的图形表示。\n\n## 设置\n首先，让我们安装所需的包并设置我们的OpenAI API密钥（我们将使用的LLM）\n```python\n%%capture --no-stderr\n% pip install -U langgraph langchain_openai\n\nimport getpass\nimport os\n\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n\n_set_env(\"OPENAI_API_KEY\")\n```\n\n### 设置LangSmith以支持LangGraph开发\n注册LangSmith可以快速发现问题并改进您的LangGraph项目。LangSmith允许您使用跟踪数据来调试、测试和监控使用LangGraph构建的LLM应用程序 —— [了解更多关于如何开始的信息](这里)。\n\n## 收集信息\n首先，让我们定义图的一部分，该部分将收集用户需求。这将是一个带有特定系统消息的LLM调用。它将能够访问一个工具，在准备好生成提示时可以调用该工具。\n\n### 使用Pydantic与LangChain\n本笔记本使用了Pydantic v2的BaseModel，这需要langchain-core >= 0.3。如果使用langchain-core < 0.3，则会导致Pydantic v1和v2 BaseModel混合导致错误。\n\n```python\nfrom typing import List\n\nfrom langchain_core.messages import SystemMessage\nfrom langchain_openai import ChatOpenAI\n\nfrom pydantic import BaseModel\n\nAPI参考: SystemMessage | ChatOpenAI\ntemplate = \"\"\"Your job is to get information from a user about what type of prompt template they want to create.\n\nYou should get the following information from them:\n\"\"\"\n```\n\n- 提示的目标是什么\n- 将传递给提示模板的变量有哪些\n- 输出不应该做什么的任何限制\n- 输出必须遵守的任何要求\n\n如果您无法明确这些信息，请让他们澄清！不要随意猜测。\n\n在您能够明确所有信息后，调用相关的工具。\n\n\n```python\ndef get_messages_info(messages):\n    return [SystemMessage(content=template)] + messages\n\n\nclass PromptInstructions(BaseModel):\n    \"\"\"关于如何向LLM发出提示的说明。\"\"\"\n\n    objective: str\n    variables: List[str]\n    constraints: List[str]\n    requirements: List[str]\n\n\nllm = ChatOpenAI(temperature=0)\nllm_with_tool = llm.bind_tools([PromptInstructions])\n\n\ndef info_chain(state):\n    messages = get_messages_info(state[\"messages\"])\n    response = llm_with_tool.invoke(messages)\n    return {\"messages\": [response]}\n```\n\n## 生成提示\n我们现在设置将生成提示的状态。\n这需要一个单独的系统消息，以及一个函数来过滤掉工具调用之前的所有消息（因为这是前一个状态决定生成提示的时候）。\n\n```python\nfrom langchain_core.messages import AIMessage, HumanMessage, ToolMessage\n\n# 新的系统提示\nprompt_system = \"\"\"基于以下需求，编写一个好的提示模板：\n\n{reqs}\"\"\"\n\n\n# 获取提示消息的函数\n# 只获取工具调用之后的消息\ndef get_prompt_messages(messages: list):\n    tool_call = None\n    other_msgs = []\n    for m in messages:\n        if isinstance(m, AIMessage) and m.tool_calls:\n            tool_call = m.tool_calls[0][\"args\"]\n        elif isinstance(m, ToolMessage):\n            continue\n        elif tool_call is not None:\n            other_msgs.append(m)\n    return [SystemMessage(content=prompt_system.format(reqs=tool_call))] + other_msgs\n```\n```python\ndef prompt_gen_chain(state):\n    messages = get_prompt_messages(state[\"messages\"])\n    response = llm.invoke(messages)\n    return {\"messages\": [response]}\n\n# API 参考: AIMessage | HumanMessage | ToolMessage\n## 定义状态逻辑\n这是聊天机器人所处状态的逻辑。\n如果最后一条消息是工具调用，那么我们处于“提示创建者”（prompt）应该响应的状态。\n否则，如果最后一条消息不是HumanMessage，那么我们知道接下来应该是人类响应，所以我们处于END状态。\n如果最后一条消息是HumanMessage，那么如果有之前的工具调用，我们就处于提示状态。\n否则，我们处于“信息收集”（info）状态。\n\nfrom typing import Literal\n\nfrom langgraph.graph import END\n\n\ndef get_state(state):\n    messages = state[\"messages\"]\n    if isinstance(messages[-1], AIMessage) and messages[-1].tool_calls:\n        return \"add_tool_message\"\n    elif not isinstance(messages[-1], HumanMessage):\n        return END\n    return \"info\"\n\n# API 参考: END\n## 创建图\n我们现在可以创建图。\n我们将使用SqliteSaver来持久化对话历史记录。\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import StateGraph, START\nfrom langgraph.graph.message import add_messages\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\n\n\nclass State(TypedDict):\n    messages: Annotated[list, add_messages]\n\n\nmemory = MemorySaver()\nworkflow = StateGraph(State)\nworkflow.add_node(\"info\", info_chain)\nworkflow.add_node(\"prompt\", prompt_gen_chain)\n\n\n@workflow.add_node\ndef add_tool_message(state: State):\n    return {\n        \"messages\": [\n            ToolMessage(\n                content=\"Prompt generated!\",\n                tool_call_id=state[\"messages\"][-1].tool_calls[0][\"id\"],\n            )\n        ]\n    }\n```\n```markdown\nworkflow.add_conditional_edges(\"info\", get_state, [\"add_tool_message\", \"info\", END])\nworkflow.add_edge(\"add_tool_message\", \"prompt\")\nworkflow.add_edge(\"prompt\", END)\nworkflow.add_edge(START, \"info\")\ngraph = workflow.compile(checkpointer=memory)\n\n# API 参考: MemorySaver | StateGraph | START | add_messages\nfrom IPython.display import Image, display\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n\n## 使用图\n我们现在可以使用创建的聊天机器人。\nimport uuid\n\ncached_human_responses = [\"hi!\", \"rag prompt\", \"1 rag, 2 none, 3 no, 4 no\", \"red\", \"q\"]\ncached_response_index = 0\nconfig = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\nwhile True:\n    try:\n        user = input(\"用户 (q/Q 退出): \")\n    except:\n        user = cached_human_responses[cached_response_index]\n        cached_response_index += 1\n    print(f\"用户 (q/Q 退出): {user}\")\n    if user in {\"q\", \"Q\"}:\n        print(\"AI: 再见\")\n        break\n    output = None\n    for output in graph.stream(\n        {\"messages\": [HumanMessage(content=user)]}, config=config, stream_mode=\"updates\"\n    ):\n        last_message = next(iter(output.values()))[\"messages\"][-1]\n        last_message.pretty_print()\n\n    if output and \"prompt\" in output:\n        print(\"完成!\")\n\n用户 (q/Q 退出): hi!\n==================================\u001B[1m Ai 消息 \u001B[0m==================================\n\n你好！今天我能帮你什么？\n\n用户 (q/Q 退出): rag prompt\n==================================\u001B[1m Ai 消息 \u001B[0m==================================\n\n当然！我可以帮助你创建一个提示模板。首先，请提供以下信息：\n\n1. 提示的目标是什么？\n2. 将传递到提示模板中的变量有哪些？\n3. 输出不应做什么的任何约束条件？\n4. 输出必须遵守的任何要求？\n```\n```markdown\n一旦我获得了这些信息，我就可以帮助你创建提示模板。\n用户 (q/Q 退出): 1 rag, 2 none, 3 no, 4 no\n==================================\u001B[1m Ai 消息 \u001B[0m==================================\n工具调用:\n  PromptInstructions (call_tcz0foifsaGKPdZmsZxNnepl)\n 调用 ID: call_tcz0foifsaGKPdZmsZxNnepl\n  参数:\n    目标: rag\n    变量: ['none']\n    约束条件: ['no']\n    要求: ['no']\n=================================\u001B[1m 工具消息 \u001B[0m=================================\n\n提示已生成！\n==================================\u001B[1m Ai 消息 \u001B[0m==================================\n\n请使用 RAG（红色、琥珀色、绿色）评级系统编写回复。\n完成!\n用户 (q/Q 退出): red\n==================================\u001B[1m Ai 消息 \u001B[0m==================================\n\n回复: 状态为红色。\n用户 (q/Q 退出): q\nAI: 再见\n\n\n\n\n        这个页面有帮助吗？\n      \n\n\n\n\n\n\n\n\n\n\n\n              \n              \n                \n              \n              感谢您的反馈！\n            \n\n              \n              \n                \n              \n              感谢您的反馈！请通过在下面的讨论中添加内容来帮助我们改进此页面。\n            \n\n\n\n\n评论\n```"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
